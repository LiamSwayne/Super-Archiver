name: 6-hour Archive

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:

jobs:
  archive:
    runs-on: ubuntu-latest

    steps:
    - name: Archive URLs
      run: |
        archival_urls=(
          "https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D0%B0%D1%8F_%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B8%D1%86%D0%B0"
          "https://en.uncyclopedia.co/wiki/Main_Page"
          "https://en.uncyclopedia.co/wiki/Special:Statistics"
          "https://arc.net/"
          "https://www.atlasobscura.com/"
          "https://www.openculture.com/"
          "https://longreads.com/"
        )

        for url in "${archival_urls[@]}"; do
          # Retry loop for the main content
          max_retries=3
          retry_counter=0
          while [ $retry_counter -lt $max_retries ]; do
            if curl -sS "https://web.archive.org/save/$url" > /dev/null; then
              break
            else
              ((retry_counter++))
              echo "Failed to archive main page: $url. Retrying..."
            fi
          done || true

          # Retry loop for outlinks
          curl -sS "$url" | grep -oP 'href="\K[^"]+' | while read -r outlink; do
            max_retries=3
            retry_counter=0
            while [ $retry_counter -lt $max_retries ]; do
              if curl -sS "https://web.archive.org/save/$outlink" > /dev/null; then
                break
              else
                ((retry_counter++))
                echo "Failed to archive outlink: $outlink. Retrying..."
              fi
            done || true
          done
        done
